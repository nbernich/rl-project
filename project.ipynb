{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Approaches to the Iterated Prisoner's Dilemma\n",
    "\n",
    "This project demonstrates how simple reinforcement learning algorithms can be used to approach the iterated prisoner's dilemma problem. It is inspired by [this popular Veritasium video](https://www.youtube.com/watch?v=mScpHTIi-kM).\n",
    "\n",
    "Collaborators:\n",
    "- Nate Bernich\n",
    "- Andrew Martin\n",
    "- Ben Borges\n",
    "- Jonathan Ranger\n",
    "\n",
    "*Submitted for CS 751 - Reinforcement Learning at UNH*\n",
    "\n",
    "## MDP Modeling\n",
    "\n",
    "The iterated prisoner's dilemma involves two opponents playing the prisoner's dilemma for a fixed number of rounds, where each opponent aims to maximize their own cumulative score.\n",
    "\n",
    "We will test different modelings of the problem, and use value iteration to compute a policy that will play against known strategies.\n",
    "\n",
    "Two things will always be the same for our purposes:\n",
    "- The two actions, to cooperate and defect\n",
    "- The reward matrix, which is shown below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "import itertools\n",
    "import random\n",
    "\n",
    "actions = [\"C\", \"D\"]  # Cooperate, Defect\n",
    "rewards = {\n",
    "    \"CC\": (3, 3), # We cooperate, they cooperate, both earn 3 points\n",
    "    \"CD\": (0, 5), # We cooperate, they defect, they earn 5 points, we earn 0\n",
    "    \"DC\": (5, 0), # etc...\n",
    "    \"DD\": (1, 1),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Previous-Round States\n",
    "\n",
    "We will start by considering a state space where each state represents the most recent action taken by the player and the opponent strategy. This gives us four states to work with.\n",
    "\n",
    "The new state in a transition is therefore formed from the chosen actions, and the reward is based on those actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [a1 + a2 for a1 in actions for a2 in actions]\n",
    "\n",
    "def transition(state, action, opponent_action):\n",
    "    return action + opponent_action\n",
    "\n",
    "\n",
    "def reward(state, action, opponent_action):\n",
    "    return rewards[action + opponent_action][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also define some utilities for printing value functions, policies, and the results of a simulated game."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_v(v):\n",
    "  print(\"Value function:\")\n",
    "  for state in states:\n",
    "    print(f\"State {state} | Value {v[state]:.4f}\")\n",
    "\n",
    "def print_policy(policy):\n",
    "  print(\"Policy:\")\n",
    "  for state in states:\n",
    "    print(f\"State {state} | Action {policy[state]}\")\n",
    "\n",
    "def print_game(player_points, opponent_points, player_hist, opponent_hist):\n",
    "  print(\"Game results:\")\n",
    "  print(f\"Player: {player_points} points\")\n",
    "  print(player_hist)\n",
    "  print()\n",
    "  print(f\"Opponent: {opponent_points} points\")\n",
    "  print(opponent_hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tit-for-tat Training\n",
    "\n",
    "We will now train a policy against an opponent strategy, tit-for-tat.\n",
    "\n",
    "This strategy has been shown to perform quite well against other strategies. It will retaliate once after every defection by the player (an eye for an eye)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tit_for_tat(state):\n",
    "    return state[0] # repeat player's action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform value iteration to determine the values of each state and the optimal action we should take at each one when playing against tit-for-tat.\n",
    "\n",
    "We see that cooperating is best against this opponent (considering only the previous actions taken at any given time), since this will avoid retaliation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State CC | Value 60.0000\n",
      "State CD | Value 60.0000\n",
      "State DC | Value 57.0000\n",
      "State DD | Value 57.0000\n",
      "\n",
      "Policy:\n",
      "State CC | Action C\n",
      "State CD | Action C\n",
      "State DC | Action C\n",
      "State DD | Action C\n"
     ]
    }
   ],
   "source": [
    "def compute_policy(opponent_strategy, iterations=1000, discount_factor=0.95):\n",
    "  v_opt = {state: 0 for state in states}\n",
    "  policy_opt = {state: None for state in states}\n",
    "\n",
    "  for _ in range(iterations):\n",
    "    new_v_opt = v_opt.copy()\n",
    "    for state in states:\n",
    "\n",
    "      # Find which action at this state maximizes value\n",
    "      max_value = float(\"-inf\")\n",
    "      best_action = None\n",
    "\n",
    "      for player_action in actions:\n",
    "        opponent_action = opponent_strategy(state)\n",
    "        next_state = transition(state, player_action, opponent_action) # next state comes from our action and the opponent's\n",
    "\n",
    "        immediate_reward = reward(state, player_action, opponent_action) # comes from the reward matrix\n",
    "        value = immediate_reward + discount_factor * v_opt[next_state]\n",
    "\n",
    "        if value > max_value:\n",
    "          max_value = value\n",
    "          best_action = player_action\n",
    "\n",
    "      new_v_opt[state] = max_value\n",
    "      policy_opt[state] = best_action\n",
    "\n",
    "    v_opt = new_v_opt\n",
    "\n",
    "  return v_opt, policy_opt\n",
    "\n",
    "v_opt, policy_opt = compute_policy(opponent_strategy=tit_for_tat)\n",
    "print_v(v_opt)\n",
    "print()\n",
    "print_policy(policy_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can play a sample game to see how this performs in practice.\n",
    "\n",
    "We will assume that both our policy and the opponent strategy begin with the assumption that they both would have cooperated before the game. We provide an option to customize this \"initial history\" which decides the first round of the actual game. As we will show, this will be better than picking arbitrary first actions when we want to test for specific results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game results:\n",
      "Player: 150 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "\n",
      "Opponent: 150 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "def play_game(policy, opponent_strategy, initial_player_hist=\"C\", initial_opponent_hist=\"C\", rounds=50):\n",
    "  player_hist = initial_player_hist[-1] # only allow one round in history to be included\n",
    "  opponent_hist = initial_opponent_hist[-1]\n",
    "  player_points = 0\n",
    "  opponent_points = 0\n",
    "\n",
    "  for _ in range(rounds):\n",
    "    last_moves = player_hist[-1] + opponent_hist[-1]\n",
    "\n",
    "    player_action = policy[last_moves]\n",
    "    opponent_action = opponent_strategy(last_moves)\n",
    "\n",
    "    reward = rewards[player_action + opponent_action]\n",
    "    player_points += reward[0]\n",
    "    opponent_points += reward[1]\n",
    "\n",
    "    player_hist += player_action\n",
    "    opponent_hist += opponent_action\n",
    "\n",
    "  return player_points, opponent_points, player_hist[1:], opponent_hist[1:] # exclude initial history from results\n",
    "\n",
    "results = play_game(\n",
    "  policy=policy_opt,\n",
    "  opponent_strategy=tit_for_tat\n",
    ")\n",
    "print_game(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: We should really defect in the last round, but since the policy is unaware of when the last round is, this is not expected behavior)\n",
    "\n",
    "Even if we defect first and trigger one retaliation, our policy rights itself to prevent future retaliation, even though it will lose points in the first actual round. This seems like what we would expect from a sensible solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game results:\n",
      "Player: 147 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "\n",
      "Opponent: 152 points\n",
      "DCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "results = play_game(\n",
    "    policy=policy_opt,\n",
    "    opponent_strategy=tit_for_tat,\n",
    "    initial_player_hist=\"D\",\n",
    "    initial_opponent_hist=\"C\"\n",
    ")\n",
    "print_game(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discount Factor Adjustment\n",
    "\n",
    "We observe that with a very low discount factor (such that future rewards are negligible), defecting is advantageous. This makes sense since we don't have to worry about retaliation if the future is not weighed highly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State CC | Value 5.1111\n",
      "State CD | Value 5.1111\n",
      "State DC | Value 1.1111\n",
      "State DD | Value 1.1111\n",
      "\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n"
     ]
    }
   ],
   "source": [
    "v_disc_future, policy_disc_future = compute_policy(opponent_strategy=tit_for_tat, discount_factor=0.1)\n",
    "\n",
    "print_v(v_disc_future)\n",
    "print()\n",
    "print_policy(policy_disc_future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the discount factor increases from $\\gamma = 0.1$ up to $\\gamma = 1.0$, we become more cooperative as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gamma = 0.0\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n",
      "\n",
      "gamma = 0.1\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n",
      "\n",
      "gamma = 0.2\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n",
      "\n",
      "gamma = 0.3\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "gamma = 0.4\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "gamma = 0.5\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "gamma = 0.6\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "gamma = 0.7\n",
      "Policy:\n",
      "State CC | Action C\n",
      "State CD | Action C\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "gamma = 0.8\n",
      "Policy:\n",
      "State CC | Action C\n",
      "State CD | Action C\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "gamma = 0.9\n",
      "Policy:\n",
      "State CC | Action C\n",
      "State CD | Action C\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "gamma = 1.0\n",
      "Policy:\n",
      "State CC | Action C\n",
      "State CD | Action C\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(11):\n",
    "    gamma = i / 10 # from 1/10 to 10/10\n",
    "    _, policy_disc_future = compute_policy(discount_factor=gamma, opponent_strategy=tit_for_tat)\n",
    "    print(f\"gamma = {gamma}\")\n",
    "    print_policy(policy_disc_future)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The interesting result is around the middle area, $\\gamma = 0.3$ to $\\gamma = 0.6$. We defect after our cooperation to gain an immediate win against tit-for-tat in the next round. However, we also sacrifice the next round after our defection by cooperating, in order to right the course and make tit-for-tat cooperate with us in the long term. Thus, the middle-ground discount factors seem to favor contraditory goals that result in a constant back-and-forth with the opponent strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action C\n",
      "State DD | Action C\n",
      "\n",
      "Game results:\n",
      "Player: 125 points\n",
      "DCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDC\n",
      "\n",
      "Opponent: 125 points\n",
      "CDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCD\n"
     ]
    }
   ],
   "source": [
    "_, policy_mid_discount = compute_policy(discount_factor=0.3, opponent_strategy=tit_for_tat)\n",
    "print_policy(policy_mid_discount)\n",
    "print()\n",
    "\n",
    "results = play_game(policy=policy_mid_discount, opponent_strategy=tit_for_tat)\n",
    "print_game(*results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbitrary Action Opponents\n",
    "\n",
    "Playing against opponent strategies with arbitrary action choices, the value function and policies are varied. However, our player is significantly better on average and is mostly able to learn the strategy well enough to earn singificant point totals. This indicates some basic success of our RL approach in its current form.\n",
    "\n",
    "Note that the goal is not to beat the opponent's score, but to maximize points in general. It just happens that winning more points often involves taking advantage of the opponent. So, we argue that the high difference between our policy's points and the opponent strategies' points can be considered a good indicator for success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 0: 150 - 25\n",
      "Game 1: 250 - 0\n",
      "Game 2: 150 - 25\n",
      "Game 3: 50 - 50\n",
      "Game 4: 246 - 1\n",
      "Game 5: 50 - 50\n",
      "Game 6: 250 - 0\n",
      "Game 7: 50 - 50\n",
      "Game 8: 125 - 125\n",
      "Game 9: 50 - 50\n",
      "...\n",
      "Overall statistics for 500 games played:\n",
      "Average player points: 172.874\n",
      "Average opponent points: 43.494\n",
      "Average difference (player - opponent): 129.38\n"
     ]
    }
   ],
   "source": [
    "def score_random_games(game_count=500, discount_factor=0.95, training_iterations=500):\n",
    "  player_results = []\n",
    "  opponent_results = []\n",
    "  result_diffs = []\n",
    "\n",
    "  for i in range(game_count):\n",
    "\n",
    "    arbitrary_policy = {state: random.choice(actions) for state in states}\n",
    "    \n",
    "    def arbitrary_strategy(state):\n",
    "      return arbitrary_policy[state]\n",
    "\n",
    "    _, policy = compute_policy(\n",
    "        iterations=training_iterations,\n",
    "        discount_factor=discount_factor,\n",
    "        opponent_strategy=arbitrary_strategy\n",
    "      )\n",
    "\n",
    "    player_points, opponent_points, _, _ = play_game(policy, arbitrary_strategy)\n",
    "\n",
    "    if i < 10:\n",
    "      print(f\"Game {i}: {player_points} - {opponent_points}\")\n",
    "\n",
    "    player_results.append(player_points)\n",
    "    opponent_results.append(opponent_points)\n",
    "    result_diffs.append(player_points - opponent_points)\n",
    "\n",
    "\n",
    "  print(\"...\")\n",
    "  print(f\"Overall statistics for {game_count} games played:\")\n",
    "  print(f\"Average player points: {statistics.mean(player_results)}\")\n",
    "  print(f\"Average opponent points: {statistics.mean(opponent_results)}\")\n",
    "  print(f\"Average difference (player - opponent): {statistics.mean(result_diffs)}\")\n",
    "\n",
    "score_random_games()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pacifist Opponent\n",
    "\n",
    "When playing against a pacifist opponent, we will always defect. This gives us the maximum number of points possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State CC | Value 100.0000\n",
      "State CD | Value 100.0000\n",
      "State DC | Value 100.0000\n",
      "State DD | Value 100.0000\n",
      "\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n",
      "\n",
      "Game results:\n",
      "Player: 250 points\n",
      "DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD\n",
      "\n",
      "Opponent: 0 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "def pacifist(state):\n",
    "    return \"C\"\n",
    "\n",
    "v_pacifist, policy_pacifist = compute_policy(opponent_strategy=pacifist)\n",
    "\n",
    "print_v(v_pacifist)\n",
    "print()\n",
    "print_policy(policy_pacifist)\n",
    "\n",
    "print()\n",
    "pacifist_results = play_game(policy_pacifist, pacifist)\n",
    "print_game(*pacifist_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggressor Opponent\n",
    "\n",
    "When playing against an opponent who always defects, we will always defect since we have no better option. Cooperating will earn the player far fewer points since the opponent would take advantage of them every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State CC | Value 20.0000\n",
      "State CD | Value 20.0000\n",
      "State DC | Value 20.0000\n",
      "State DD | Value 20.0000\n",
      "\n",
      "Policy:\n",
      "State CC | Action D\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n",
      "\n",
      "Game results:\n",
      "Player: 50 points\n",
      "DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD\n",
      "\n",
      "Opponent: 50 points\n",
      "DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD\n"
     ]
    }
   ],
   "source": [
    "def agressor(state):\n",
    "    return \"D\"\n",
    "\n",
    "v_aggressor, policy_aggressor = compute_policy(opponent_strategy=agressor)\n",
    "\n",
    "print_v(v_aggressor)\n",
    "print()\n",
    "print_policy(policy_aggressor)\n",
    "\n",
    "print()\n",
    "results_aggressor = play_game(policy_aggressor, agressor)\n",
    "print_game(*results_aggressor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friedman\n",
    "\n",
    "Another common and interesting opponent strategy is called Friedman. This strategy will cooperate until the player defects once. Then, it will defect the rest of the game.\n",
    "\n",
    "We have devised a way to implement this using states that contain only the previous round:\n",
    "\n",
    "1. If the player defected last, we defect.\n",
    "\n",
    "2. If we defected before, defect again. At some point, our repeat defections must have started and we need to continue them.\n",
    "\n",
    "3. Otherwise, cooperate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State CC | Value 60.0000\n",
      "State CD | Value 20.0000\n",
      "State DC | Value 20.0000\n",
      "State DD | Value 20.0000\n",
      "\n",
      "Policy:\n",
      "State CC | Action C\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n"
     ]
    }
   ],
   "source": [
    "def friedman(state):\n",
    "    if state[0] == \"D\":\n",
    "        return \"D\"\n",
    "\n",
    "    if state[1] == \"D\":\n",
    "        return \"D\"\n",
    "\n",
    "    return \"C\"\n",
    "\n",
    "v_friedman, policy_friedman = compute_policy(opponent_strategy=friedman)\n",
    "\n",
    "print_v(v_friedman)\n",
    "print()\n",
    "print_policy(policy_friedman)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we assume cooperation initially, Friedman will cooperate with us all the way through."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game results:\n",
      "Player: 150 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n",
      "\n",
      "Opponent: 150 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "friedman_results = play_game(\n",
    "    policy_friedman,\n",
    "    friedman,\n",
    "    initial_player_hist=\"C\",\n",
    "    initial_opponent_hist=\"C\"\n",
    ")\n",
    "print_game(*friedman_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we defect, Friedman will defect the rest of the way too. We respond by defecting for the same reason considered when playing against the aggressor strategy. So, it seems our policy is making the right decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game results:\n",
      "Player: 50 points\n",
      "DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD\n",
      "\n",
      "Opponent: 50 points\n",
      "DDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDDD\n"
     ]
    }
   ],
   "source": [
    "friedman_results_2 = play_game(\n",
    "    policy_friedman,\n",
    "    friedman,\n",
    "    initial_player_hist=\"D\",\n",
    "    initial_opponent_hist=\"C\"\n",
    ")\n",
    "print_game(*friedman_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joss\n",
    "\n",
    "The algorithm for Joss is the following:\n",
    "\n",
    "1. Defect `10%` of the time.\n",
    "\n",
    "2. Otherwise, copy the player's last move."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joss(state):\n",
    "    if random.random() < 0.1:\n",
    "        return \"D\" # sneaky move\n",
    "    \n",
    "    return state[-1][0] # copies the player"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems we usually beat or tie Joss. However, subsequent runs prove that the results vary drastically.\n",
    "\n",
    "This exposes a potential weakness when faced against strategies that involve random actions which may be different from the random actions that we trained on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State CC | Value 95.3457\n",
      "State CD | Value 20.0000\n",
      "State DC | Value 97.3457\n",
      "State DD | Value 20.0000\n",
      "\n",
      "Policy:\n",
      "State CC | Action C\n",
      "State CD | Action D\n",
      "State DC | Action D\n",
      "State DD | Action D\n",
      "\n",
      "Game results:\n",
      "Player: 105 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCDDDDDDDDDDDDDDDDDDDDD\n",
      "\n",
      "Opponent: 110 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCDDDDDDDDDDDDDDDDDDDDDD\n"
     ]
    }
   ],
   "source": [
    "v_joss, policy_joss = compute_policy(opponent_strategy=joss)\n",
    "\n",
    "print_v(v_joss)\n",
    "print()\n",
    "print_policy(policy_joss)\n",
    "print()\n",
    "\n",
    "joss_results = play_game(\n",
    "    policy_joss,\n",
    "    joss,\n",
    "    initial_player_hist=\"C\",\n",
    "    initial_opponent_hist=\"C\"\n",
    ")\n",
    "print_game(*joss_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $k$-History\n",
    "\n",
    "We next consider what would happen when we expand the state space by representing the most recent $k$ rounds (actions from both the player and opponent) in the state instead of the single most recent.\n",
    "\n",
    "This enables us to experiment with different strategies that look back further than one round. We call this \"$k$-history\" and our examples will use $k = 3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of states for k = 3: 64\n",
      "('CC', 'CC', 'CC')\n",
      "('CC', 'CC', 'CD')\n",
      "('CC', 'CC', 'DC')\n",
      "('CC', 'CC', 'DD')\n",
      "('CC', 'CD', 'CC')\n",
      "('CC', 'CD', 'CD')\n",
      "('CC', 'CD', 'DC')\n",
      "('CC', 'CD', 'DD')\n",
      "('CC', 'DC', 'CC')\n",
      "('CC', 'DC', 'CD')\n",
      "('CC', 'DC', 'DC')\n",
      "('CC', 'DC', 'DD')\n",
      "('CC', 'DD', 'CC')\n",
      "('CC', 'DD', 'CD')\n",
      "('CC', 'DD', 'DC')\n",
      "('CC', 'DD', 'DD')\n",
      "('CD', 'CC', 'CC')\n",
      "('CD', 'CC', 'CD')\n",
      "('CD', 'CC', 'DC')\n",
      "('CD', 'CC', 'DD')\n",
      "('CD', 'CD', 'CC')\n",
      "('CD', 'CD', 'CD')\n",
      "('CD', 'CD', 'DC')\n",
      "('CD', 'CD', 'DD')\n",
      "('CD', 'DC', 'CC')\n",
      "('CD', 'DC', 'CD')\n",
      "('CD', 'DC', 'DC')\n",
      "('CD', 'DC', 'DD')\n",
      "('CD', 'DD', 'CC')\n",
      "('CD', 'DD', 'CD')\n",
      "('CD', 'DD', 'DC')\n",
      "('CD', 'DD', 'DD')\n",
      "('DC', 'CC', 'CC')\n",
      "('DC', 'CC', 'CD')\n",
      "('DC', 'CC', 'DC')\n",
      "('DC', 'CC', 'DD')\n",
      "('DC', 'CD', 'CC')\n",
      "('DC', 'CD', 'CD')\n",
      "('DC', 'CD', 'DC')\n",
      "('DC', 'CD', 'DD')\n",
      "('DC', 'DC', 'CC')\n",
      "('DC', 'DC', 'CD')\n",
      "('DC', 'DC', 'DC')\n",
      "('DC', 'DC', 'DD')\n",
      "('DC', 'DD', 'CC')\n",
      "('DC', 'DD', 'CD')\n",
      "('DC', 'DD', 'DC')\n",
      "('DC', 'DD', 'DD')\n",
      "('DD', 'CC', 'CC')\n",
      "('DD', 'CC', 'CD')\n",
      "('DD', 'CC', 'DC')\n",
      "('DD', 'CC', 'DD')\n",
      "('DD', 'CD', 'CC')\n",
      "('DD', 'CD', 'CD')\n",
      "('DD', 'CD', 'DC')\n",
      "('DD', 'CD', 'DD')\n",
      "('DD', 'DC', 'CC')\n",
      "('DD', 'DC', 'CD')\n",
      "('DD', 'DC', 'DC')\n",
      "('DD', 'DC', 'DD')\n",
      "('DD', 'DD', 'CC')\n",
      "('DD', 'DD', 'CD')\n",
      "('DD', 'DD', 'DC')\n",
      "('DD', 'DD', 'DD')\n"
     ]
    }
   ],
   "source": [
    "k = 3\n",
    "possible_rounds = [\"\".join(perm) for perm in itertools.product(actions, repeat=2)]\n",
    "k_hist_states = list(itertools.product(possible_rounds, repeat=k))\n",
    "\n",
    "print(f\"Number of states for k = {k}: {len(k_hist_states)}\")\n",
    "for k_hist_state in k_hist_states:\n",
    "    print(k_hist_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We redefine the value function and policy display utilities to work with this modeling. While the reward remains the same, the transition is a bit different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_hist_print_v(v):\n",
    "  print(\"Value function:\")\n",
    "  for k_hist_state in k_hist_states:\n",
    "    print(f\"State {k_hist_state} | Value {v[k_hist_state]:.4f}\")\n",
    "\n",
    "def k_hist_print_policy(policy):\n",
    "  print(\"Policy:\")\n",
    "  for k_hist_state in k_hist_states:\n",
    "    print(f\"State {k_hist_state} | Action {policy[k_hist_state]}\")\n",
    "\n",
    "def k_hist_transition(k_hist_state, player_action, opponent_action):\n",
    "    new_round = player_action + opponent_action\n",
    "    return k_hist_state[1:] + (new_round,) # most recent two rounds + the new round\n",
    "\n",
    "def k_hist_reward(player_action, opponent_action):\n",
    "    return rewards[player_action + opponent_action][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tit-for-two-tats\n",
    "\n",
    "We consider a new strategy, tit-for-two-tats. This requires $k \\ge 2$ and will defect only if the player defects twice in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tit_for_two_tats(k_hist_state):\n",
    "    if k_hist_state[-1][0] == \"D\" and k_hist_state[-2][0] == \"D\":\n",
    "        return \"D\"\n",
    "    return \"C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use a modified value iteration approach to work with $k$-history. The resulting policy is evidently much more complex than the one trained to oppose tit-for-tat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State ('CC', 'CC', 'CC') | Value 80.5128\n",
      "State ('CC', 'CC', 'CD') | Value 80.5128\n",
      "State ('CC', 'CC', 'DC') | Value 79.4872\n",
      "State ('CC', 'CC', 'DD') | Value 79.4872\n",
      "State ('CC', 'CD', 'CC') | Value 80.5128\n",
      "State ('CC', 'CD', 'CD') | Value 80.5128\n",
      "State ('CC', 'CD', 'DC') | Value 79.4872\n",
      "State ('CC', 'CD', 'DD') | Value 79.4872\n",
      "State ('CC', 'DC', 'CC') | Value 80.5128\n",
      "State ('CC', 'DC', 'CD') | Value 80.5128\n",
      "State ('CC', 'DC', 'DC') | Value 76.4872\n",
      "State ('CC', 'DC', 'DD') | Value 76.4872\n",
      "State ('CC', 'DD', 'CC') | Value 80.5128\n",
      "State ('CC', 'DD', 'CD') | Value 80.5128\n",
      "State ('CC', 'DD', 'DC') | Value 76.4872\n",
      "State ('CC', 'DD', 'DD') | Value 76.4872\n",
      "State ('CD', 'CC', 'CC') | Value 80.5128\n",
      "State ('CD', 'CC', 'CD') | Value 80.5128\n",
      "State ('CD', 'CC', 'DC') | Value 79.4872\n",
      "State ('CD', 'CC', 'DD') | Value 79.4872\n",
      "State ('CD', 'CD', 'CC') | Value 80.5128\n",
      "State ('CD', 'CD', 'CD') | Value 80.5128\n",
      "State ('CD', 'CD', 'DC') | Value 79.4872\n",
      "State ('CD', 'CD', 'DD') | Value 79.4872\n",
      "State ('CD', 'DC', 'CC') | Value 80.5128\n",
      "State ('CD', 'DC', 'CD') | Value 80.5128\n",
      "State ('CD', 'DC', 'DC') | Value 76.4872\n",
      "State ('CD', 'DC', 'DD') | Value 76.4872\n",
      "State ('CD', 'DD', 'CC') | Value 80.5128\n",
      "State ('CD', 'DD', 'CD') | Value 80.5128\n",
      "State ('CD', 'DD', 'DC') | Value 76.4872\n",
      "State ('CD', 'DD', 'DD') | Value 76.4872\n",
      "State ('DC', 'CC', 'CC') | Value 80.5128\n",
      "State ('DC', 'CC', 'CD') | Value 80.5128\n",
      "State ('DC', 'CC', 'DC') | Value 79.4872\n",
      "State ('DC', 'CC', 'DD') | Value 79.4872\n",
      "State ('DC', 'CD', 'CC') | Value 80.5128\n",
      "State ('DC', 'CD', 'CD') | Value 80.5128\n",
      "State ('DC', 'CD', 'DC') | Value 79.4872\n",
      "State ('DC', 'CD', 'DD') | Value 79.4872\n",
      "State ('DC', 'DC', 'CC') | Value 80.5128\n",
      "State ('DC', 'DC', 'CD') | Value 80.5128\n",
      "State ('DC', 'DC', 'DC') | Value 76.4872\n",
      "State ('DC', 'DC', 'DD') | Value 76.4872\n",
      "State ('DC', 'DD', 'CC') | Value 80.5128\n",
      "State ('DC', 'DD', 'CD') | Value 80.5128\n",
      "State ('DC', 'DD', 'DC') | Value 76.4872\n",
      "State ('DC', 'DD', 'DD') | Value 76.4872\n",
      "State ('DD', 'CC', 'CC') | Value 80.5128\n",
      "State ('DD', 'CC', 'CD') | Value 80.5128\n",
      "State ('DD', 'CC', 'DC') | Value 79.4872\n",
      "State ('DD', 'CC', 'DD') | Value 79.4872\n",
      "State ('DD', 'CD', 'CC') | Value 80.5128\n",
      "State ('DD', 'CD', 'CD') | Value 80.5128\n",
      "State ('DD', 'CD', 'DC') | Value 79.4872\n",
      "State ('DD', 'CD', 'DD') | Value 79.4872\n",
      "State ('DD', 'DC', 'CC') | Value 80.5128\n",
      "State ('DD', 'DC', 'CD') | Value 80.5128\n",
      "State ('DD', 'DC', 'DC') | Value 76.4872\n",
      "State ('DD', 'DC', 'DD') | Value 76.4872\n",
      "State ('DD', 'DD', 'CC') | Value 80.5128\n",
      "State ('DD', 'DD', 'CD') | Value 80.5128\n",
      "State ('DD', 'DD', 'DC') | Value 76.4872\n",
      "State ('DD', 'DD', 'DD') | Value 76.4872\n",
      "\n",
      "Policy:\n",
      "State ('CC', 'CC', 'CC') | Action D\n",
      "State ('CC', 'CC', 'CD') | Action D\n",
      "State ('CC', 'CC', 'DC') | Action C\n",
      "State ('CC', 'CC', 'DD') | Action C\n",
      "State ('CC', 'CD', 'CC') | Action D\n",
      "State ('CC', 'CD', 'CD') | Action D\n",
      "State ('CC', 'CD', 'DC') | Action C\n",
      "State ('CC', 'CD', 'DD') | Action C\n",
      "State ('CC', 'DC', 'CC') | Action D\n",
      "State ('CC', 'DC', 'CD') | Action D\n",
      "State ('CC', 'DC', 'DC') | Action C\n",
      "State ('CC', 'DC', 'DD') | Action C\n",
      "State ('CC', 'DD', 'CC') | Action D\n",
      "State ('CC', 'DD', 'CD') | Action D\n",
      "State ('CC', 'DD', 'DC') | Action C\n",
      "State ('CC', 'DD', 'DD') | Action C\n",
      "State ('CD', 'CC', 'CC') | Action D\n",
      "State ('CD', 'CC', 'CD') | Action D\n",
      "State ('CD', 'CC', 'DC') | Action C\n",
      "State ('CD', 'CC', 'DD') | Action C\n",
      "State ('CD', 'CD', 'CC') | Action D\n",
      "State ('CD', 'CD', 'CD') | Action D\n",
      "State ('CD', 'CD', 'DC') | Action C\n",
      "State ('CD', 'CD', 'DD') | Action C\n",
      "State ('CD', 'DC', 'CC') | Action D\n",
      "State ('CD', 'DC', 'CD') | Action D\n",
      "State ('CD', 'DC', 'DC') | Action C\n",
      "State ('CD', 'DC', 'DD') | Action C\n",
      "State ('CD', 'DD', 'CC') | Action D\n",
      "State ('CD', 'DD', 'CD') | Action D\n",
      "State ('CD', 'DD', 'DC') | Action C\n",
      "State ('CD', 'DD', 'DD') | Action C\n",
      "State ('DC', 'CC', 'CC') | Action D\n",
      "State ('DC', 'CC', 'CD') | Action D\n",
      "State ('DC', 'CC', 'DC') | Action C\n",
      "State ('DC', 'CC', 'DD') | Action C\n",
      "State ('DC', 'CD', 'CC') | Action D\n",
      "State ('DC', 'CD', 'CD') | Action D\n",
      "State ('DC', 'CD', 'DC') | Action C\n",
      "State ('DC', 'CD', 'DD') | Action C\n",
      "State ('DC', 'DC', 'CC') | Action D\n",
      "State ('DC', 'DC', 'CD') | Action D\n",
      "State ('DC', 'DC', 'DC') | Action C\n",
      "State ('DC', 'DC', 'DD') | Action C\n",
      "State ('DC', 'DD', 'CC') | Action D\n",
      "State ('DC', 'DD', 'CD') | Action D\n",
      "State ('DC', 'DD', 'DC') | Action C\n",
      "State ('DC', 'DD', 'DD') | Action C\n",
      "State ('DD', 'CC', 'CC') | Action D\n",
      "State ('DD', 'CC', 'CD') | Action D\n",
      "State ('DD', 'CC', 'DC') | Action C\n",
      "State ('DD', 'CC', 'DD') | Action C\n",
      "State ('DD', 'CD', 'CC') | Action D\n",
      "State ('DD', 'CD', 'CD') | Action D\n",
      "State ('DD', 'CD', 'DC') | Action C\n",
      "State ('DD', 'CD', 'DD') | Action C\n",
      "State ('DD', 'DC', 'CC') | Action D\n",
      "State ('DD', 'DC', 'CD') | Action D\n",
      "State ('DD', 'DC', 'DC') | Action C\n",
      "State ('DD', 'DC', 'DD') | Action C\n",
      "State ('DD', 'DD', 'CC') | Action D\n",
      "State ('DD', 'DD', 'CD') | Action D\n",
      "State ('DD', 'DD', 'DC') | Action C\n",
      "State ('DD', 'DD', 'DD') | Action C\n"
     ]
    }
   ],
   "source": [
    "def k_hist_compute_policy(opponent_strategy, iterations=1000, discount_factor=0.95):\n",
    "    v_opt = {k_hist_state: 0 for k_hist_state in k_hist_states}\n",
    "    policy_opt = {k_hist_state: None for k_hist_state in k_hist_states}\n",
    "\n",
    "    for _ in range(iterations):\n",
    "        new_v_opt = v_opt.copy()\n",
    "        for k_hist_state in k_hist_states:\n",
    "\n",
    "            max_value = float(\"-inf\")\n",
    "            best_action = None\n",
    "\n",
    "            for player_action in actions:\n",
    "\n",
    "                opponent_action = opponent_strategy(k_hist_state)\n",
    "                next_state = k_hist_transition(k_hist_state, player_action, opponent_action)\n",
    "\n",
    "                immediate_reward = k_hist_reward(player_action, opponent_action)\n",
    "                value = immediate_reward + discount_factor * v_opt[next_state]\n",
    "\n",
    "                if value > max_value:\n",
    "                    max_value = value\n",
    "                    best_action = player_action\n",
    "\n",
    "            new_v_opt[k_hist_state] = max_value\n",
    "            policy_opt[k_hist_state] = best_action\n",
    "\n",
    "        v_opt = new_v_opt\n",
    "\n",
    "    return v_opt, policy_opt\n",
    "\n",
    "two_tats_v, two_tats_policy = k_hist_compute_policy(\n",
    "    opponent_strategy=tit_for_two_tats\n",
    ")\n",
    "\n",
    "k_hist_print_v(two_tats_v)\n",
    "print()\n",
    "k_hist_print_policy(two_tats_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We gain a better understanding of the policy by simulating a game. It seems that the policy will defect every other round to gain the maximum number of points in those rounds. However, it never defects twice in a row since this would trigger the revenge mechanism."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game results:\n",
      "Player: 200 points\n",
      "DCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDC\n",
      "\n",
      "Opponent: 75 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "def k_hist_play_game(policy, opponent_strategy, initial_player_hist=\"CCC\", initial_opponent_hist=\"CCC\", rounds=50):\n",
    "    player_hist = initial_player_hist\n",
    "    opponent_hist = initial_opponent_hist\n",
    "\n",
    "    player_points = 0\n",
    "    opponent_points = 0\n",
    "\n",
    "    for _ in range(rounds):\n",
    "\n",
    "        k_hist_state = tuple(\"\".join(pair) for pair in zip(player_hist[-k:], opponent_hist[-k:])) # collect the most recent k rounds from game history\n",
    "\n",
    "        # Player selects action based on policy\n",
    "        player_action = policy[k_hist_state]\n",
    "\n",
    "        # Opponent selects action based on their strategy\n",
    "        opponent_action = opponent_strategy(k_hist_state)\n",
    "\n",
    "        # Calculate rewards\n",
    "        reward_pair = rewards[player_action + opponent_action]\n",
    "        player_points += reward_pair[0]\n",
    "        opponent_points += reward_pair[1]\n",
    "\n",
    "        # Update histories\n",
    "        player_hist += player_action\n",
    "        opponent_hist += opponent_action\n",
    "\n",
    "    return player_points, opponent_points, player_hist[3:], opponent_hist[3:] # truncate initial history just as before\n",
    "\n",
    "# Play the game using the optimal policy against tit-for-tat\n",
    "two_tats_results = k_hist_play_game(\n",
    "    two_tats_policy,\n",
    "    tit_for_two_tats\n",
    ")\n",
    "print_game(*two_tats_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even when we start with two defections to force the revenge mechanism, our policy will quickly right the course, only losing minimal points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game results:\n",
      "Player: 197 points\n",
      "CDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCDCD\n",
      "\n",
      "Opponent: 77 points\n",
      "DCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "two_tats_results_2 = k_hist_play_game(\n",
    "    two_tats_policy,\n",
    "    tit_for_two_tats,\n",
    "    initial_player_hist=\"CDD\",\n",
    "    initial_opponent_hist=\"CCC\"\n",
    ")\n",
    "print_game(*two_tats_results_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tit-for-$k$-tats\n",
    "\n",
    "We can extend the opponent strategy to one we will call \"tit-for-$k$-tats\". It will only defect when the player defects $k$ times in a row (every round observable in the state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tit_for_k_tats(k_hist_state):\n",
    "    if all(round[0] == \"D\" for round in k_hist_state):\n",
    "        return \"D\"\n",
    "    return \"C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting policy is a bit different, defecting much more often."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "State ('CC', 'CC', 'CC') | Value 87.3444\n",
      "State ('CC', 'CC', 'CD') | Value 87.3444\n",
      "State ('CC', 'CC', 'DC') | Value 86.6784\n",
      "State ('CC', 'CC', 'DD') | Value 86.6784\n",
      "State ('CC', 'CD', 'CC') | Value 87.3444\n",
      "State ('CC', 'CD', 'CD') | Value 87.3444\n",
      "State ('CC', 'CD', 'DC') | Value 86.6784\n",
      "State ('CC', 'CD', 'DD') | Value 86.6784\n",
      "State ('CC', 'DC', 'CC') | Value 87.3444\n",
      "State ('CC', 'DC', 'CD') | Value 87.3444\n",
      "State ('CC', 'DC', 'DC') | Value 85.9772\n",
      "State ('CC', 'DC', 'DD') | Value 85.9772\n",
      "State ('CC', 'DD', 'CC') | Value 87.3444\n",
      "State ('CC', 'DD', 'CD') | Value 87.3444\n",
      "State ('CC', 'DD', 'DC') | Value 85.9772\n",
      "State ('CC', 'DD', 'DD') | Value 85.9772\n",
      "State ('CD', 'CC', 'CC') | Value 87.3444\n",
      "State ('CD', 'CC', 'CD') | Value 87.3444\n",
      "State ('CD', 'CC', 'DC') | Value 86.6784\n",
      "State ('CD', 'CC', 'DD') | Value 86.6784\n",
      "State ('CD', 'CD', 'CC') | Value 87.3444\n",
      "State ('CD', 'CD', 'CD') | Value 87.3444\n",
      "State ('CD', 'CD', 'DC') | Value 86.6784\n",
      "State ('CD', 'CD', 'DD') | Value 86.6784\n",
      "State ('CD', 'DC', 'CC') | Value 87.3444\n",
      "State ('CD', 'DC', 'CD') | Value 87.3444\n",
      "State ('CD', 'DC', 'DC') | Value 85.9772\n",
      "State ('CD', 'DC', 'DD') | Value 85.9772\n",
      "State ('CD', 'DD', 'CC') | Value 87.3444\n",
      "State ('CD', 'DD', 'CD') | Value 87.3444\n",
      "State ('CD', 'DD', 'DC') | Value 85.9772\n",
      "State ('CD', 'DD', 'DD') | Value 85.9772\n",
      "State ('DC', 'CC', 'CC') | Value 87.3444\n",
      "State ('DC', 'CC', 'CD') | Value 87.3444\n",
      "State ('DC', 'CC', 'DC') | Value 86.6784\n",
      "State ('DC', 'CC', 'DD') | Value 86.6784\n",
      "State ('DC', 'CD', 'CC') | Value 87.3444\n",
      "State ('DC', 'CD', 'CD') | Value 87.3444\n",
      "State ('DC', 'CD', 'DC') | Value 86.6784\n",
      "State ('DC', 'CD', 'DD') | Value 86.6784\n",
      "State ('DC', 'DC', 'CC') | Value 87.3444\n",
      "State ('DC', 'DC', 'CD') | Value 87.3444\n",
      "State ('DC', 'DC', 'DC') | Value 82.9772\n",
      "State ('DC', 'DC', 'DD') | Value 82.9772\n",
      "State ('DC', 'DD', 'CC') | Value 87.3444\n",
      "State ('DC', 'DD', 'CD') | Value 87.3444\n",
      "State ('DC', 'DD', 'DC') | Value 82.9772\n",
      "State ('DC', 'DD', 'DD') | Value 82.9772\n",
      "State ('DD', 'CC', 'CC') | Value 87.3444\n",
      "State ('DD', 'CC', 'CD') | Value 87.3444\n",
      "State ('DD', 'CC', 'DC') | Value 86.6784\n",
      "State ('DD', 'CC', 'DD') | Value 86.6784\n",
      "State ('DD', 'CD', 'CC') | Value 87.3444\n",
      "State ('DD', 'CD', 'CD') | Value 87.3444\n",
      "State ('DD', 'CD', 'DC') | Value 86.6784\n",
      "State ('DD', 'CD', 'DD') | Value 86.6784\n",
      "State ('DD', 'DC', 'CC') | Value 87.3444\n",
      "State ('DD', 'DC', 'CD') | Value 87.3444\n",
      "State ('DD', 'DC', 'DC') | Value 82.9772\n",
      "State ('DD', 'DC', 'DD') | Value 82.9772\n",
      "State ('DD', 'DD', 'CC') | Value 87.3444\n",
      "State ('DD', 'DD', 'CD') | Value 87.3444\n",
      "State ('DD', 'DD', 'DC') | Value 82.9772\n",
      "State ('DD', 'DD', 'DD') | Value 82.9772\n",
      "\n",
      "Policy:\n",
      "State ('CC', 'CC', 'CC') | Action D\n",
      "State ('CC', 'CC', 'CD') | Action D\n",
      "State ('CC', 'CC', 'DC') | Action D\n",
      "State ('CC', 'CC', 'DD') | Action D\n",
      "State ('CC', 'CD', 'CC') | Action D\n",
      "State ('CC', 'CD', 'CD') | Action D\n",
      "State ('CC', 'CD', 'DC') | Action D\n",
      "State ('CC', 'CD', 'DD') | Action D\n",
      "State ('CC', 'DC', 'CC') | Action D\n",
      "State ('CC', 'DC', 'CD') | Action D\n",
      "State ('CC', 'DC', 'DC') | Action C\n",
      "State ('CC', 'DC', 'DD') | Action C\n",
      "State ('CC', 'DD', 'CC') | Action D\n",
      "State ('CC', 'DD', 'CD') | Action D\n",
      "State ('CC', 'DD', 'DC') | Action C\n",
      "State ('CC', 'DD', 'DD') | Action C\n",
      "State ('CD', 'CC', 'CC') | Action D\n",
      "State ('CD', 'CC', 'CD') | Action D\n",
      "State ('CD', 'CC', 'DC') | Action D\n",
      "State ('CD', 'CC', 'DD') | Action D\n",
      "State ('CD', 'CD', 'CC') | Action D\n",
      "State ('CD', 'CD', 'CD') | Action D\n",
      "State ('CD', 'CD', 'DC') | Action D\n",
      "State ('CD', 'CD', 'DD') | Action D\n",
      "State ('CD', 'DC', 'CC') | Action D\n",
      "State ('CD', 'DC', 'CD') | Action D\n",
      "State ('CD', 'DC', 'DC') | Action C\n",
      "State ('CD', 'DC', 'DD') | Action C\n",
      "State ('CD', 'DD', 'CC') | Action D\n",
      "State ('CD', 'DD', 'CD') | Action D\n",
      "State ('CD', 'DD', 'DC') | Action C\n",
      "State ('CD', 'DD', 'DD') | Action C\n",
      "State ('DC', 'CC', 'CC') | Action D\n",
      "State ('DC', 'CC', 'CD') | Action D\n",
      "State ('DC', 'CC', 'DC') | Action D\n",
      "State ('DC', 'CC', 'DD') | Action D\n",
      "State ('DC', 'CD', 'CC') | Action D\n",
      "State ('DC', 'CD', 'CD') | Action D\n",
      "State ('DC', 'CD', 'DC') | Action D\n",
      "State ('DC', 'CD', 'DD') | Action D\n",
      "State ('DC', 'DC', 'CC') | Action D\n",
      "State ('DC', 'DC', 'CD') | Action D\n",
      "State ('DC', 'DC', 'DC') | Action C\n",
      "State ('DC', 'DC', 'DD') | Action C\n",
      "State ('DC', 'DD', 'CC') | Action D\n",
      "State ('DC', 'DD', 'CD') | Action D\n",
      "State ('DC', 'DD', 'DC') | Action C\n",
      "State ('DC', 'DD', 'DD') | Action C\n",
      "State ('DD', 'CC', 'CC') | Action D\n",
      "State ('DD', 'CC', 'CD') | Action D\n",
      "State ('DD', 'CC', 'DC') | Action D\n",
      "State ('DD', 'CC', 'DD') | Action D\n",
      "State ('DD', 'CD', 'CC') | Action D\n",
      "State ('DD', 'CD', 'CD') | Action D\n",
      "State ('DD', 'CD', 'DC') | Action D\n",
      "State ('DD', 'CD', 'DD') | Action D\n",
      "State ('DD', 'DC', 'CC') | Action D\n",
      "State ('DD', 'DC', 'CD') | Action D\n",
      "State ('DD', 'DC', 'DC') | Action C\n",
      "State ('DD', 'DC', 'DD') | Action C\n",
      "State ('DD', 'DD', 'CC') | Action D\n",
      "State ('DD', 'DD', 'CD') | Action D\n",
      "State ('DD', 'DD', 'DC') | Action C\n",
      "State ('DD', 'DD', 'DD') | Action C\n"
     ]
    }
   ],
   "source": [
    "k_tats_v, k_tats_policy = k_hist_compute_policy(\n",
    "    opponent_strategy=tit_for_k_tats\n",
    ")\n",
    "\n",
    "k_hist_print_v(k_tats_v)\n",
    "print()\n",
    "k_hist_print_policy(k_tats_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simulation of the game shows, the player under this policy will again defect as much as they can get away with, without triggering the revenge mechanism. This ultimately shows that $k$-history modeling will allow us to compute policies that effectively counter more complicated opponent strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game results:\n",
      "Player: 218 points\n",
      "DDCDDCDDCDDCDDCDDCDDCDDCDDCDDCDDCDDCDDCDDCDDCDDCDD\n",
      "\n",
      "Opponent: 48 points\n",
      "CCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCCC\n"
     ]
    }
   ],
   "source": [
    "k_tats_results = k_hist_play_game(\n",
    "    k_tats_policy,\n",
    "    tit_for_k_tats,\n",
    "    initial_player_hist=\"CCC\",\n",
    "    initial_opponent_hist=\"CCC\"\n",
    ")\n",
    "print_game(*k_tats_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Our testing has shown that simple RL algorithms produce desirable results when trained against deterministic opponent strategies. Extensions such as $k$-history also allow us to simulate more complex deterministic opponents, which our policies are still able to adequately play against. We only found that opponents with randomness may not allow us to compute an optimal policy. The weakness arises from a disconnect between the random actions chosen in training, and those chosen in a simulation.\n",
    "\n",
    "We conclude that our approach is an interesting case study for how RL can be utilized effectively in relatively simple problems, and how certain parameters and modeling can be tuned to explore different results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
